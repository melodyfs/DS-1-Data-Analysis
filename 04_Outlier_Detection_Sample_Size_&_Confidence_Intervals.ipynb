{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src='img/ms_logo.jpeg' height=40% width=40%></center>\n",
    "\n",
    "<center><h1>Outlier Detection, Sample Size, and Confidence Intervals</h1></center>\n",
    "\n",
    "When you're designing an experiment, numbers matter.  After all, we want out experiments to be statistically valid--otherwise, we're just guessing.  In this notebook, we'll learn a method for detecting outliers in our data set called \"Tukey Fences\", named after famed statistician John Tukey.  \n",
    "\n",
    "Next, we'll learn about confidence inteverals, sample size, and the relationship between the two.  We'll learn how to calculate confidence intervals based on sample size, as well as how to determine the minimum sample size needed in order to reach a specific confidence interval.  \n",
    "\n",
    "Let's get started!\n",
    "\n",
    "<center><h2>Outlier Detection</h2></center>\n",
    "\n",
    "Recall that before we begin an experiment, we usually start by \"cleaning\" our dataset.  This step usually includes things like:\n",
    "\n",
    "* Exploring our dataset(s) to get a feel for what changes need to be made to make it more usable\n",
    "* Examining and standardizing the values within cells (converting \"yes\"/\"no\" answers to 1's and 0's, for example)\n",
    "* Dealing with cells that contain NaNs (Null values)\n",
    "* Organizing and structuring datasets as needed (for instance, combining many small datasets into one big one)\n",
    "* Normalizing continuous data into z-scores with a mean of 0 and unit variance.  \n",
    "\n",
    "Another major step we need to do at this point in the project is to detect **outliers**, and determine how to deal wit them.  Outliers are extreme values that can skew our dataset, sometimes giving us an incorrect picture of how things actually are in our dataset.  The hardest part of this is determining which data points are acceptable, and which ones constitute \"outlier\" status.  This is where \"Tukey Fences\" come into play!\n",
    "\n",
    "### 1.5 x IQR\n",
    "\n",
    "In order to find outliers, we first need a working definition of what constitutes an outlier.  Tukey suggested we calculate the range between the first quartile (25%) and  third quartile (75%) in the data, called the **interquartile range**.  We then multiply this value by 1.5.  To get the Fence for high values, add this value to the Q3 value.  Anything greater than this \"Fence\" value is considered an outlier.  Similarly, to get the Fence for low values, subtract 1.5 x IQR from Q1.  Anything less than this \"Fence\" value is also considered an outlier.  \n",
    "\n",
    "Let's try an example!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(1547)\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a random normal distribution of 1000 samples with mean 100 and std_dev of 8\n",
    "normal_dist = np.random.normal(100, 8, (1000)).astype('float64')\n",
    "# Generate a random uniform distribution between 1 and 200 with 100 samples\n",
    "uniform_dist = np.random.uniform(1, 200, (100)).astype('float64')\n",
    "# Combine both distributions and store in and Pandas Series object\n",
    "sample_dset = pd.Series(np.append(normal_dist, uniform_dist))\n",
    "sample_dset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've created an ugly data set, let's see if we can identify some outliers.  \n",
    "\n",
    "Start by calculating the **Inter-Quartile Range**: Q3 - Q1.\n",
    "\n",
    "Next, calculate how far your fences are from the quartiles: f = IQR x 1.5\n",
    "\n",
    "Finally, place your fences and filter for values outside them:  Lower Fence = Q1 - f, Upper Fence = Q3 + f\n",
    "\n",
    "See if you can write write some code to filter for outliers in the `sample_dset` array we've just created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Locations for Q1 and Q3\n",
    "q1 = None\n",
    "q3 = None\n",
    "\n",
    "# calculate fence locations\n",
    "lower_fence = None\n",
    "upper_fence = None\n",
    "\n",
    "# Filter out the outliers and inspect them!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! That works, but it isn't efficient to calculate this manually every time we run across a new data set.  \n",
    "\n",
    "**TASK:** Write a function that takes in a pandas series, and returns a new pandas series with the outliers removed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_outliers(series):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h2>Sample Size and Confidence Intervals</h2></center>\n",
    "\n",
    "## What is a Confidence Interval?\n",
    "\n",
    "Recall that in statistics, we almost never get to work with the entire population.  Instead, we work with samples taken from the population, and use statistical methods to try and estimates about the population based on what we see in the samples. When you think about this estimation process, this leads to two very important questions:\n",
    "\n",
    "<center>1. **_How accurate are our estimates?_**</center>\n",
    "<br>\n",
    "<center>2. **_How many samples do we need to be need to be sure our estimates are accurate?_**</center>\n",
    "\n",
    "This is where confidence intervals come in to play.  When estimating population parameters such as the population mean, for example, it is impossible to know with certainty that our estimate is 100% accurate.  Instead, statisticians define an acceptable margin of error.  In plain English, that means that we're okay with our estimate being wrong, as long as we're {X}% sure we're within a certain distance from the mean.  \n",
    "\n",
    "To illustrate this concept, let's look at a type of graph statisticians use to denote confidence intervals, called a **_Box Plot_**.  \n",
    "\n",
    "<center><img src='http://www.cs.utsa.edu/~cs1173/lessons/BoxPlotQuestions/BoxPlotQuestions_02.png'></center>\n",
    "\n",
    "This is a box plot of the confidence intervals for the population means of 3 different types of Iris flowers (you'll get very familiar with this data set when you move onto supervised learning).  The only way that we could know the true mean of the sepal length of these three species of Irises is if we took the time to record the sepal length of every one of them *in the world*.  This isn't plausible.  Instead, we can use the data we've collected about our samples to determine upper and lower bounds for our confidence interval.  If we have an acceptable error rate (often refered to as an 'Alpha' value) of 5%, then that means that we have a confidence interval of 95%.  This means that we are 95% confident that the actual value of the population mean (often called the 'ground truth') is between our upper and lower bounds, which we find by using the confidence interval formula.  \n",
    "\n",
    "<center><img src='img/Confidence_Interval_Formulas.png' height=60% width=60%></center>\n",
    "\n",
    "Don't let the mathematical notation in those pictures scare you.  Here's what they each mean:\n",
    "\n",
    "n = sample size\n",
    "<br>\n",
    "x_bar = mean of the sample\n",
    "<br>\n",
    "s = standard deviation of the sample\n",
    "<br>\n",
    "z* = point probability for that percentage (can be found with a lookup table or using the `scipy.stats` package)\n",
    "\n",
    "**TASK:**  Read in the `iris.csv` data sets from the dataset folder. Make sure you specific that `header=None`, and se the `column names` variable to set the column names.  Compute the confidence intervals for at least one type of Iris flower.  \n",
    "\n",
    "**STRETCH CHALLENGE #1:**  Write a function that takes in a Pandas Series and confidence level and returns the confidence interval.  (Hint: remember that each column in a dataframe is just a Series object!)\n",
    "\n",
    "**STRETCH CHALLENGE #2:** Pick a column and visualize the the sample mean or median for at least one flower using a box-whisker plot.   (Hint: Consider writing the function from the first challenge to output everything needed for this visualization--then, this will be really easy!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the dataset from iris.csv, in the datasets folder.  Make sure you pass 'header=None' and 'names=column_names'\n",
    "# when calling pd.read_csv()!\n",
    "column_names = ['Sepal Length(cm)', 'Sepal Width(cm)', 'Petal Length(cm)', 'Petal Width(cm)', 'Class']\n",
    "df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as st\n",
    "sample_size = None\n",
    "sample_mean = None\n",
    "sample_std_dev = None\n",
    "z_star = st.norm.interval(0.95)\n",
    "ucl = None\n",
    "lcl = None"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:MS-ML-P3]",
   "language": "python",
   "name": "conda-env-MS-ML-P3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
